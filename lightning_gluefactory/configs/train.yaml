defaults:
  - loggers/aimstack@logger
  - callbacks:
      - model_checkpointing
      - learning_rate_monitor

train:
  seed: "???"
  epochs: 1
  opt_regexp:  # regular expression to filter parameters to optimize
  optimizer_options: { } # options for the optimizer
  optimizer: # optional arguments passed to the optimizer
    _target_: torch.optim.Adam
  lr: 0.001 # learning rate
  lr_schedule:
    type:
    start: 0
    exp_div_10: 0
    on_epoch: false
    factor: 1.0
  lr_scaling:
   -
    - 100
    - "dampingnet.const"

  eval_every_iter: 1000  # interval for evaluation on the validation set
  save_every_iter: 5000  # interval for saving the current checkpoint
  log_every_iter: 200  # interval for logging the loss to the console
  log_grad_every_iter:   # interval for logging gradient hists
  test_every_epoch: 1  # interval for evaluation on the test benchmarks
  keep_last_checkpoints: 10  # keep only the last X checkpoints
  load_experiment:   # initialize the model from a previous experiment
  median_metrics: [ ]  # add the median of some metrics
  recall_metrics: { }  # add the recall of some metrics
  pr_metrics: { }  # add pr curves, set labels/predictions/mask keys
  best_key: "loss/total"  # key to use to select the best checkpoint
  dataset_callback_fn:   # data func called at the start of each epoch
  dataset_callback_on_val: false  # call data func on val data?
  clip_grad:
  pr_curves: { }
  plot:
  submodules: [ ]
  strategy: auto
  train_metric_prefix: train/
  val_metric_prefix: val/
callbacks:
  model_checkpointing:
    monitor: "val/loss/total"  # key to use to select the best checkpoint
    mode: "min"  # min/max
overfit: false